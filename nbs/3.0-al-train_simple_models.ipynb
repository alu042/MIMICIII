{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train simple models for prediction of In-hospital mortality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression\n",
    "- Random forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.structured import *\n",
    "from fastai.column_data import *\n",
    "import yaml\n",
    "from pandas import DataFrame, Series\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_DIR = %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = '/data1/MIMIC-III/RAW/'\n",
    "INTERIM_DATA = f'{RAW_DATA}/../interim/'\n",
    "PROCESSED_DATA = f'{RAW_DATA}/../processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIMIC3_BENCHMARK_LOCATION = f'{NB_DIR}/../mimic3-benchmarks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='in-hospital-mortality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_patients = set()\n",
    "with open(f\"{MIMIC3_BENCHMARK_LOCATION}/mimic3models/valset.csv\", \"r\") as valset_file:\n",
    "    for line in valset_file:\n",
    "        x, y = line.split(',')\n",
    "        if int(y) == 1:\n",
    "            val_patients.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PROCESSED_DATA}/{task}/train/listfile.csv\") as listfile:\n",
    "    lines = listfile.readlines()\n",
    "    header = lines[0]\n",
    "    lines = lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [x for x in lines if x[:x.find(\"_\")] not in val_patients]\n",
    "val_lines = [x for x in lines if x[:x.find(\"_\")] in val_patients]\n",
    "assert len(train_lines) + len(val_lines) == len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PROCESSED_DATA}/{task}/train_listfile.csv\", \"w\") as train_listfile:\n",
    "    train_listfile.write(header)\n",
    "    for line in train_lines:\n",
    "        train_listfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{PROCESSED_DATA}/{task}/val_listfile.csv\", \"w\") as val_listfile:\n",
    "    val_listfile.write(header)\n",
    "    for line in val_lines:\n",
    "        val_listfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data1/MIMIC-III/RAW//../processed//in-hospital-mortality/test_listfile.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(f\"{PROCESSED_DATA}/{task}/test/listfile.csv\",\n",
    "            f\"{PROCESSED_DATA}/{task}/test_listfile.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunk(reader, chunk_size):\n",
    "    data = {}\n",
    "    for i in range(chunk_size):\n",
    "        ret = reader.read_next()\n",
    "        for k, v in iter(ret.items()):\n",
    "            if k not in data:\n",
    "                data[k] = []\n",
    "            data[k].append(v)\n",
    "    data[\"header\"] = data[\"header\"][0]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_rawdata(chunk, header, period, features):\n",
    "    with open(os.path.join(f'{MIMIC3_BENCHMARK_LOCATION}/resources/', \"channel_info.json\")) as channel_info_file:\n",
    "        channel_info = json.loads(channel_info_file.read())\n",
    "    data = [convert_to_dict(X, header, channel_info) for X in chunk]\n",
    "    return extract_features(data, period, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dict(data, header, channel_info):\n",
    "    \"\"\" convert data from readers output in to array of arrays format \"\"\"\n",
    "    ret = [[] for i in range(data.shape[1] - 1)]\n",
    "    for i in range(1, data.shape[1]):\n",
    "        ret[i-1] = [(t, x) for (t, x) in zip(data[:, 0], data[:, i]) if x != \"\"]\n",
    "        channel = header[i]\n",
    "        if (len(channel_info[channel]['possible_values']) != 0):\n",
    "            ret[i-1] = list(map(lambda x: (x[0], channel_info[channel]['values'][x[1]]), ret[i-1])) # list(..) for Python3\n",
    "        ret[i-1] = list(map(lambda x: (float(x[0]), float(x[1])), ret[i-1]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "all_functions = [min, max, np.mean, np.std, skew, len]\n",
    "\n",
    "functions_map = {\n",
    "    \"all\": all_functions,\n",
    "    \"len\": [len],\n",
    "    \"all_but_len\": all_functions[:-1]\n",
    "}\n",
    "\n",
    "periods_map = {\n",
    "    \"all\": (0, 0, 1, 0),\n",
    "    \"first4days\": (0, 0, 0, 4 * 24),\n",
    "    \"first8days\": (0, 0, 0, 8 * 24),\n",
    "    \"last12hours\": (1, -12, 1, 0),\n",
    "    \"first25percent\": (2, 25),\n",
    "    \"first50percent\": (2, 50)\n",
    "}\n",
    "\n",
    "sub_periods = [(2, 100), (2, 10), (2, 25), (2, 50),\n",
    "               (3, 10), (3, 25), (3, 50)]\n",
    "\n",
    "\n",
    "def get_range(begin, end, period):\n",
    "    # first p %\n",
    "    if period[0] == 2:\n",
    "        return (begin, begin + (end - begin) * period[1] / 100.0)\n",
    "    # last p %\n",
    "    if period[0] == 3:\n",
    "        return (end - (end - begin) * period[1] / 100.0, end)\n",
    "\n",
    "    if period[0] == 0:\n",
    "        L = begin + period[1]\n",
    "    else:\n",
    "        L = end + period[1]\n",
    "\n",
    "    if period[2] == 0:\n",
    "        R = begin + period[3]\n",
    "    else:\n",
    "        R = end + period[3]\n",
    "\n",
    "    return (L, R)\n",
    "\n",
    "\n",
    "def calculate(channel_data, period, sub_period, functions):\n",
    "    if len(list(channel_data)) == 0:\n",
    "        return np.full((len(functions, )), np.nan)\n",
    "\n",
    "    L = channel_data[0][0]\n",
    "    R = channel_data[-1][0]\n",
    "    L, R = get_range(L, R, period)\n",
    "    L, R = get_range(L, R, sub_period)\n",
    "\n",
    "    data = [x for (t, x) in channel_data\n",
    "            if L - 1e-6 < t < R + 1e-6]\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return np.full((len(functions, )), np.nan)\n",
    "    return np.array([fn(data) for fn in functions], dtype=np.float32)\n",
    "\n",
    "\n",
    "def extract_features_single_episode(data_raw, period, functions):\n",
    "    global sub_periods\n",
    "    extracted_features = [np.concatenate([calculate(data_raw[i], period, sub_period, functions)\n",
    "                                          for sub_period in sub_periods],\n",
    "                                         axis=0)\n",
    "                          for i in range(len(data_raw))]\n",
    "    return np.concatenate(extracted_features, axis=0)\n",
    "\n",
    "\n",
    "def extract_features(data_raw, period, features):\n",
    "    period = periods_map[period]\n",
    "    functions = functions_map[features]\n",
    "    return np.array([extract_features_single_episode(x, period, functions)\n",
    "                     for x in data_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_extract_features(reader, period, features):\n",
    "    ret = read_chunk(reader, reader.get_number_of_examples())\n",
    "    # ret = common_utils.read_chunk(reader, 100)\n",
    "    X = extract_features_from_rawdata(ret['X'], ret['header'], period, features)\n",
    "    return (X, ret['y'], ret['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = \"all\" # choices=['first4days', 'first8days', 'last12hours', 'first25percent', 'first50percent', 'all']\n",
    "features = \"all\" # choices=['all', 'len', 'all_but_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader(object):\n",
    "    def __init__(self, dataset_dir, listfile=None):\n",
    "        self._dataset_dir = dataset_dir\n",
    "        self._current_index = 0\n",
    "        if listfile is None:\n",
    "            listfile_path = os.path.join(dataset_dir, \"listfile.csv\")\n",
    "        else:\n",
    "            listfile_path = listfile\n",
    "        with open(listfile_path, \"r\") as lfile:\n",
    "            self._data = lfile.readlines()\n",
    "        self._listfile_header = self._data[0]\n",
    "        self._data = self._data[1:]\n",
    "\n",
    "    def get_number_of_examples(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def random_shuffle(self, seed=None):\n",
    "        if (seed is not None):\n",
    "            random.seed(seed)\n",
    "        random.shuffle(self._data)\n",
    "\n",
    "    def read_example(self, index):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def read_next(self):\n",
    "        to_read_index = self._current_index\n",
    "        self._current_index += 1\n",
    "        if (self._current_index == self.get_number_of_examples()):\n",
    "            self._current_index = 0\n",
    "        return self.read_example(to_read_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InHospitalMortalityReader(Reader):\n",
    "    def __init__(self, dataset_dir, listfile=None, period_length=48.0):\n",
    "        \"\"\" Reader for in-hospital moratality prediction task.\n",
    "        :param dataset_dir:   Directory where timeseries files are stored.\n",
    "        :param listfile:      Path to a listfile. If this parameter is left `None` then\n",
    "                              `dataset_dir/listfile.csv` will be used.\n",
    "        :param period_length: Length of the period (in hours) from which the prediction is done.\n",
    "        \"\"\"\n",
    "        Reader.__init__(self, dataset_dir, listfile)\n",
    "        self._data = [line.split(',') for line in self._data]\n",
    "        self._data = [(x, int(y)) for (x, y) in self._data]\n",
    "        self._period_length = period_length\n",
    "\n",
    "    def _read_timeseries(self, ts_filename):\n",
    "        ret = []\n",
    "        with open(os.path.join(self._dataset_dir, ts_filename), \"r\") as tsfile:\n",
    "            header = tsfile.readline().strip().split(',')\n",
    "            assert header[0] == \"Hours\"\n",
    "            for line in tsfile:\n",
    "                mas = line.strip().split(',')\n",
    "                ret.append(np.array(mas))\n",
    "        return (np.stack(ret), header)\n",
    "\n",
    "    def read_example(self, index):\n",
    "        \"\"\" Reads the example with given index.\n",
    "        :param index: Index of the line of the listfile to read (counting starts from 0).\n",
    "        :return: Dictionary with the following keys:\n",
    "            X : np.array\n",
    "                2D array containing all events. Each row corresponds to a moment.\n",
    "                First column is the time and other columns correspond to different\n",
    "                variables.\n",
    "            t : float\n",
    "                Length of the data in hours. Note, in general, it is not equal to the\n",
    "                timestamp of last event.\n",
    "            y : int (0 or 1)\n",
    "                In-hospital mortality.\n",
    "            header : array of strings\n",
    "                Names of the columns. The ordering of the columns is always the same.\n",
    "            name: Name of the sample.\n",
    "        \"\"\"\n",
    "        if (index < 0 or index >= len(self._data)):\n",
    "            raise ValueError(\"Index must be from 0 (inclusive) to number of lines (exclusive).\")\n",
    "\n",
    "        name = self._data[index][0]\n",
    "        t = self._period_length\n",
    "        y = self._data[index][1]\n",
    "        (X, header) = self._read_timeseries(name)\n",
    "\n",
    "        return {\"X\": X,\n",
    "                \"t\": t,\n",
    "                \"y\": y,\n",
    "                \"header\": header,\n",
    "                \"name\": name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reader = InHospitalMortalityReader(dataset_dir=f'{PROCESSED_DATA}/in-hospital-mortality/train/',\n",
    "                                             listfile=f'{PROCESSED_DATA}/in-hospital-mortality/train_listfile.csv',\n",
    "                                             period_length=48.0)\n",
    "\n",
    "val_reader = InHospitalMortalityReader(dataset_dir=f'{PROCESSED_DATA}/in-hospital-mortality/train/',\n",
    "                                           listfile=f'{PROCESSED_DATA}/in-hospital-mortality/val_listfile.csv',\n",
    "                                           period_length=48.0)\n",
    "\n",
    "test_reader = InHospitalMortalityReader(dataset_dir=f'{PROCESSED_DATA}/in-hospital-mortality/test/',\n",
    "                                            listfile=f'{PROCESSED_DATA}/in-hospital-mortality/test_listfile.csv',\n",
    "                                            period_length=48.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14681"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reader.get_number_of_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m           InHospitalMortalityReader\n",
       "\u001b[0;31mString form:\u001b[0m    <__main__.InHospitalMortalityReader object at 0x7fbe4f3372b0>\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Reader for in-hospital moratality prediction task.\n",
       ":param dataset_dir:   Directory where timeseries files are stored.\n",
       ":param listfile:      Path to a listfile. If this parameter is left `None` then\n",
       "                      `dataset_dir/listfile.csv` will be used.\n",
       ":param period_length: Length of the period (in hours) from which the prediction is done.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_reader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and extracting features ...\n",
      "  train data shape = (14681, 714)\n",
      "  validation data shape = (3222, 714)\n",
      "  test data shape = (3236, 714)\n"
     ]
    }
   ],
   "source": [
    "print('Reading data and extracting features ...')\n",
    "(train_X, train_y, train_names) = read_and_extract_features(train_reader, period, features)\n",
    "(val_X, val_y, val_names) = read_and_extract_features(val_reader, period, features)\n",
    "(test_X, test_y, test_names) = read_and_extract_features(test_reader, period, features)\n",
    "print('  train data shape = {}'.format(train_X.shape))\n",
    "print('  validation data shape = {}'.format(val_X.shape))\n",
    "print('  test data shape = {}'.format(test_X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAVE train, val, test arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{PROCESSED_DATA}/{task}/train_X', train_X)\n",
    "np.save(f'{PROCESSED_DATA}/{task}/train_y', train_y)\n",
    "np.save(f'{PROCESSED_DATA}/{task}/train_names', train_names)\n",
    "\n",
    "np.save(f'{PROCESSED_DATA}/{task}/val_X', val_X)\n",
    "np.save(f'{PROCESSED_DATA}/{task}/val_y', val_y)\n",
    "np.save(f'{PROCESSED_DATA}/{task}/val_names', val_names)\n",
    "\n",
    "np.save(f'{PROCESSED_DATA}/{task}/test_X', test_X)\n",
    "np.save(f'{PROCESSED_DATA}/{task}/test_y', test_y)\n",
    "np.save(f'{PROCESSED_DATA}/{task}/test_names', test_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load(f'{PROCESSED_DATA}/{task}/train_X.npy')\n",
    "train_y = np.load(f'{PROCESSED_DATA}/{task}/train_y.npy')\n",
    "train_names = np.load(f'{PROCESSED_DATA}/{task}/train_names.npy')\n",
    "\n",
    "val_X = np.load(f'{PROCESSED_DATA}/{task}/val_X.npy')\n",
    "val_y = np.load(f'{PROCESSED_DATA}/{task}/val_y.npy')\n",
    "val_names = np.load(f'{PROCESSED_DATA}/{task}/val_names.npy')\n",
    "\n",
    "\n",
    "test_X = np.load(f'{PROCESSED_DATA}/{task}/test_X.npy')\n",
    "test_y = np.load(f'{PROCESSED_DATA}/{task}/test_y.npy')\n",
    "test_names = np.load(f'{PROCESSED_DATA}/{task}/test_names.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTS (TMP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = read_chunk(train_reader, train_reader.get_number_of_examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14681"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ret['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ret['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ret['header']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_info_file = open(f'{MIMIC3_BENCHMARK_LOCATION}/resources/channel_info.json')\n",
    "channel_info = json.loads(channel_info_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [convert_to_dict(X, header, channel_info) for X in ret['X']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14681"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.001 # Inverse of L1 / L2 regularization\n",
    "l2 = True \n",
    "l1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(missing_values=np.nan, strategy='mean', axis=0,\n",
    "                  verbose=0, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values=nan, strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.array(imputer.transform(train_X), dtype=np.float32)\n",
    "val_X = np.array(imputer.transform(val_X), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = scaler.transform(train_X)\n",
    "val_X = scaler.transform(val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define logreg-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = (\"l2\" if l2 else \"l1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l2'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty=penalty, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on train and valdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob = logreg.predict_proba(train_X)\n",
    "val_prob = logreg.predict_proba(val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842721885430148"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_binary(y_true, predictions, verbose=1):\n",
    "    predictions = np.array(predictions)\n",
    "    if (len(predictions.shape) == 1):\n",
    "        predictions = np.stack([1 - predictions, predictions]).transpose((1, 0))\n",
    "\n",
    "    cf = metrics.confusion_matrix(y_true, predictions.argmax(axis=1))\n",
    "    if verbose:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(cf)\n",
    "    cf = cf.astype(np.float32)\n",
    "\n",
    "    acc = (cf[0][0] + cf[1][1]) / np.sum(cf)\n",
    "    prec0 = cf[0][0] / (cf[0][0] + cf[1][0])\n",
    "    prec1 = cf[1][1] / (cf[1][1] + cf[0][1])\n",
    "    rec0 = cf[0][0] / (cf[0][0] + cf[0][1])\n",
    "    rec1 = cf[1][1] / (cf[1][1] + cf[1][0])\n",
    "    auroc = metrics.roc_auc_score(y_true, predictions[:, 1])\n",
    "\n",
    "    (precisions, recalls, thresholds) = metrics.precision_recall_curve(y_true, predictions[:, 1])\n",
    "    auprc = metrics.auc(recalls, precisions)\n",
    "    minpse = np.max([min(x, y) for (x, y) in zip(precisions, recalls)])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"accuracy =\", acc)\n",
    "        print(\"precision class 0 =\", prec0)\n",
    "        print(\"precision class 1 =\", prec1)\n",
    "        print(\"recall class 0 =\", rec0)\n",
    "        print(\"recall class 1 =\", rec1)\n",
    "        print(\"AUC of ROC =\", auroc)\n",
    "        print(\"AUC of PRC =\", auprc)\n",
    "        print(\"min(+P, Se) =\", minpse)\n",
    "\n",
    "    return {\"acc\": acc,\n",
    "            \"prec0\": prec0,\n",
    "            \"prec1\": prec1,\n",
    "            \"rec0\": rec0,\n",
    "            \"rec1\": rec1,\n",
    "            \"auroc\": auroc,\n",
    "            \"auprc\": auprc,\n",
    "            \"minpse\": minpse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[12218   476]\n",
      " [ 1223   764]]\n",
      "accuracy = 0.8842722\n",
      "precision class 0 = 0.90900975\n",
      "precision class 1 = 0.61612904\n",
      "recall class 0 = 0.96250194\n",
      "recall class 1 = 0.38449925\n",
      "AUC of ROC = 0.8646263339721424\n",
      "AUC of PRC = 0.5495369335364912\n",
      "min(+P, Se) = 0.5288509784244857\n"
     ]
    }
   ],
   "source": [
    "acc, prec0, prec1, rec0, rec1, auroc, auprc, minpse = print_metrics_binary(train_y, train_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[2665  121]\n",
      " [ 274  162]]\n",
      "accuracy = 0.87740535\n",
      "precision class 0 = 0.906771\n",
      "precision class 1 = 0.5724382\n",
      "recall class 0 = 0.95656854\n",
      "recall class 1 = 0.37155962\n",
      "AUC of ROC = 0.8455737073308879\n",
      "AUC of PRC = 0.48928153761470383\n",
      "min(+P, Se) = 0.5160550458715596\n"
     ]
    }
   ],
   "source": [
    "acc_v, prec0_v, prec1_v, rec0_v, rec1_v, auroc_v, auprc_v, minpse_v = print_metrics_binary(val_y, val_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(imputer.transform(test_X), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob = logreg.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[2746  116]\n",
      " [ 232  142]]\n",
      "accuracy = 0.8924598\n",
      "precision class 0 = 0.92209536\n",
      "precision class 1 = 0.5503876\n",
      "recall class 0 = 0.9594689\n",
      "recall class 1 = 0.37967914\n",
      "AUC of ROC = 0.848424122841437\n",
      "AUC of PRC = 0.4743702838948688\n",
      "min(+P, Se) = 0.4613333333333333\n"
     ]
    }
   ],
   "source": [
    "acc_t, prec0_t, prec1_t, rec0_t, rec1_t, auroc_t, auprc_t, minpse_t = print_metrics_binary(test_y, test_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob_rf = rf.predict_proba(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874667938151352"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8767846058348852"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8899876390605687"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = metrics.confusion_matrix(test_y, rf.predict_proba(test_X).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2795,   67],\n",
       "       [ 289,   85]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestClassifier(n_estimators=40, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=0.5, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=3, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=-1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8992583436341162"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2795,   67],\n",
       "       [ 289,   85]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_y, rf.predict_proba(test_X).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda2/envs/fastai/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "test_proba_xg = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8983312731767614"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y, test_proba_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2797,   65],\n",
       "       [ 264,  110]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_y, model.predict_proba(test_X).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[2797   65]\n",
      " [ 264  110]]\n",
      "accuracy = 0.8983313\n",
      "precision class 0 = 0.9137537\n",
      "precision class 1 = 0.62857145\n",
      "recall class 0 = 0.9772886\n",
      "recall class 1 = 0.29411766\n",
      "AUC of ROC = 0.6357031282114524\n",
      "AUC of PRC = 0.5021356379387354\n",
      "min(+P, Se) = 0.29411764705882354\n"
     ]
    }
   ],
   "source": [
    "metrics_xg = print_metrics_binary(test_y, test_proba_xg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fast.ai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
